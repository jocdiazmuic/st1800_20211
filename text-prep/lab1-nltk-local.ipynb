{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "lab1-nltk-local.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jocdiazmuic/st1800_20211/blob/main/text-prep/lab1-nltk-local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feu8U4QiC87g"
      },
      "source": [
        "#\n",
        "# Edwin Montoya-Múnera - emontoya@eafit.edu.co\n",
        "# Universidad EAFIT \n",
        "# 2021-1\n",
        "#"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1GljLuQC87n"
      },
      "source": [
        "# cargar las librerias necesarias\n",
        "## 1. nltk para 'procesamiento natural del lenguaje'\n",
        "## 2. pandas para procesamiento de dataframes, muy usado en preparación de datos\n",
        "## 3. re - expresiones regulares\n",
        "## 4. numpy, codecs, etc - otras"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_REhBXq_C87o"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import codecs\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQkdlMVgC87o",
        "outputId": "4e26df22-f723-4b78-e6e1-0cf5c749249a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install nltk\n",
        "!pip install pyspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/67/5158f846202d7f012d1c9ca21c3549a58fd3c6707ae8ee823adcaca6473c/pyspark-3.0.2.tar.gz (204.8MB)\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 80kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 22.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186687 sha256=d6ba9f610d6597b73fdb980a21f94de460d2a489ef8cb3b5d89644772cd2c6ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/09/da/c1f2859bcc86375dc972c5b6af4881b3603269bcc4c9be5d16\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_AYT9UQDIU0"
      },
      "source": [
        "# Nueva sección"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXTXEGqGDUNV",
        "outputId": "d09793d4-0448-4b2c-f3ca-770f6841c9b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b1OF0BXC87o"
      },
      "source": [
        "# directorios (path) de entrada y salida:\n",
        "# \n",
        "path_in=\"../datasets/gutenberg-es/\"\n",
        "path_out=\"../out/\"\n",
        "filenametxt='pg2000.txt'\n",
        "filenamecleantxt='pg2000_clean.txt'\n",
        "filenamecsv='pg2000.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egG1p0f_C87p",
        "outputId": "0a5a80ad-8270-4155-b457-b20adb6874ef"
      },
      "source": [
        "# corpus de nltk para 'tokenizer' y 'stopwords'\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/emontoya/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/emontoya/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkejhQwnC87q",
        "outputId": "d0fce074-41f7-4b23-c587-f935096a7975"
      },
      "source": [
        "# ejemplo de como nltk tokeniza:\n",
        "texto=\"texto libre que permite crear     hiso20091iras epor--4 no s#e preocupe \\n hola mundo cruel\"\n",
        "tokens = nltk.word_tokenize(texto)\n",
        "print(len(tokens))\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17\n",
            "['texto', 'libre', 'que', 'permite', 'crear', 'hiso20091iras', 'epor', '--', '4', 'no', 's', '#', 'e', 'preocupe', 'hola', 'mundo', 'cruel']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1Kjp58QC87q",
        "outputId": "c4b9d2de-a67a-435a-e029-6efe40e5166f"
      },
      "source": [
        "# note la estrategia de tokenizar con sentencias simples de python, \n",
        "# ¿ cual le parece mejor?\n",
        "# y note la diferencia entre .split() y .split(' ')\n",
        "texto=\"texto libre que permite crear     hiso20091iras            epor--4 no s#e preocupe \\n hola mundo cruel\"\n",
        "tokens = texto.split()\n",
        "print(len(tokens))\n",
        "print(tokens)\n",
        "tokens = texto.split(' ')\n",
        "print(len(tokens))\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13\n",
            "['texto', 'libre', 'que', 'permite', 'crear', 'hiso20091iras', 'epor--4', 'no', 's#e', 'preocupe', 'hola', 'mundo', 'cruel']\n",
            "29\n",
            "['texto', 'libre', 'que', 'permite', 'crear', '', '', '', '', 'hiso20091iras', '', '', '', '', '', '', '', '', '', '', '', 'epor--4', 'no', 's#e', 'preocupe', '\\n', 'hola', 'mundo', 'cruel']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsUY0CBvC87q",
        "outputId": "ed1ea849-6695-49f3-b97d-b128a301c736"
      },
      "source": [
        "# stopwords con 'stop-words'\n",
        "# otra libreria diferentes de nltk para diccionario de stopwords, cual será mejor?\n",
        "# $ pip install stop-words\n",
        "# $ git clone --recursive git://github.com/Alir3z4/python-stop-words.git\n",
        "from stop_words import get_stop_words\n",
        "stop_words = get_stop_words('spanish')\n",
        "stop_words = get_stop_words('es')\n",
        "print(len(stop_words))\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "308\n",
            "['a', 'al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'como', 'con', 'contra', 'cual', 'cuando', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el', 'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas', 'estad', 'estada', 'estadas', 'estado', 'estados', 'estamos', 'estando', 'estar', 'estaremos', 'estará', 'estarán', 'estarás', 'estaré', 'estaréis', 'estaría', 'estaríais', 'estaríamos', 'estarían', 'estarías', 'estas', 'este', 'estemos', 'esto', 'estos', 'estoy', 'estuve', 'estuviera', 'estuvierais', 'estuvieran', 'estuvieras', 'estuvieron', 'estuviese', 'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuviéramos', 'estuviésemos', 'estuvo', 'está', 'estábamos', 'estáis', 'están', 'estás', 'esté', 'estéis', 'estén', 'estés', 'fue', 'fuera', 'fuerais', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen', 'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'fuéramos', 'fuésemos', 'ha', 'habida', 'habidas', 'habido', 'habidos', 'habiendo', 'habremos', 'habrá', 'habrán', 'habrás', 'habré', 'habréis', 'habría', 'habríais', 'habríamos', 'habrían', 'habrías', 'habéis', 'había', 'habíais', 'habíamos', 'habían', 'habías', 'han', 'has', 'hasta', 'hay', 'haya', 'hayamos', 'hayan', 'hayas', 'hayáis', 'he', 'hemos', 'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras', 'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses', 'hubimos', 'hubiste', 'hubisteis', 'hubiéramos', 'hubiésemos', 'hubo', 'la', 'las', 'le', 'les', 'lo', 'los', 'me', 'mi', 'mis', 'mucho', 'muchos', 'muy', 'más', 'mí', 'mía', 'mías', 'mío', 'míos', 'nada', 'ni', 'no', 'nos', 'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', 'o', 'os', 'otra', 'otras', 'otro', 'otros', 'para', 'pero', 'poco', 'por', 'porque', 'que', 'quien', 'quienes', 'qué', 'se', 'sea', 'seamos', 'sean', 'seas', 'seremos', 'será', 'serán', 'serás', 'seré', 'seréis', 'sería', 'seríais', 'seríamos', 'serían', 'serías', 'seáis', 'sido', 'siendo', 'sin', 'sobre', 'sois', 'somos', 'son', 'soy', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 'sí', 'también', 'tanto', 'te', 'tendremos', 'tendrá', 'tendrán', 'tendrás', 'tendré', 'tendréis', 'tendría', 'tendríais', 'tendríamos', 'tendrían', 'tendrías', 'tened', 'tenemos', 'tenga', 'tengamos', 'tengan', 'tengas', 'tengo', 'tengáis', 'tenida', 'tenidas', 'tenido', 'tenidos', 'teniendo', 'tenéis', 'tenía', 'teníais', 'teníamos', 'tenían', 'tenías', 'ti', 'tiene', 'tienen', 'tienes', 'todo', 'todos', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais', 'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis', 'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis', 'tuviéramos', 'tuviésemos', 'tuvo', 'tuya', 'tuyas', 'tuyo', 'tuyos', 'tú', 'un', 'una', 'uno', 'unos', 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'y', 'ya', 'yo', 'él', 'éramos']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3kOy3JPC87r",
        "outputId": "755b0369-f899-4fb1-fdab-59577910de23"
      },
      "source": [
        "# stopwords en nltk\n",
        "from nltk.corpus import stopwords\n",
        " \n",
        "stop_words_nltk = set(stopwords.words('spanish'))\n",
        "print(len(stop_words_nltk))\n",
        "print(stop_words_nltk)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313\n",
            "{'tendríais', 'ella', 'estuvieras', 'seas', 'les', 'tuvierais', 'esas', 'estuvieseis', 'estadas', 'muchos', 'hayamos', 'eras', 'fuera', 'habremos', 'sentidas', 'tuvieses', 'nuestra', 'míos', 'sí', 'tenga', 'otra', 'estas', 'estoy', 'algo', 'nos', 'estaré', 'fuiste', 'no', 'por', 'serás', 'tuyo', 'nosotros', 'sentidos', 'suyo', 'tendríamos', 'tuviese', 'fue', 'hubiéramos', 'habidos', 'y', 'habíamos', 'estarías', 'has', 'hubo', 'teníais', 'yo', 'otras', 'tuvieran', 'hubieran', 'estuvimos', 'quienes', 'tuviste', 'estarán', 'esos', 'la', 'hubiesen', 'habida', 'tendrá', 'tanto', 'estamos', 'tuviesen', 'suya', 'en', 'somos', 'tienen', 'más', 'tenida', 'el', 'tengan', 'tuvo', 'vuestra', 'tienes', 'e', 'tenido', 'fuéramos', 'estará', 'pero', 'estaba', 'con', 'a', 'tendrías', 'me', 'estuvisteis', 'habríamos', 'nosotras', 'teníamos', 'tendréis', 'estuviesen', 'te', 'estuvieron', 'fuimos', 'durante', 'han', 'todos', 'hubierais', 'ha', 'eran', 'tiene', 'hubiésemos', 'estada', 'una', 'he', 'al', 'tenían', 'estuvieses', 'os', 'estabas', 'tengamos', 'serán', 'habríais', 'soy', 'tu', 'estás', 'contra', 'ese', 'otros', 'nuestro', 'sentid', 'seríamos', 'era', 'hubiera', 'hubiese', 'algunas', 'sea', 'tuvimos', 'eres', 'mucho', 'seáis', 'seréis', 'donde', 'estado', 'habría', 'qué', 'estuvieran', 'habías', 'también', 'son', 'estad', 'ante', 'nuestros', 'tú', 'sentido', 'habrán', 'tus', 'mías', 'estados', 'hubieses', 'vuestras', 'su', 'hay', 'esa', 'tuvieras', 'estuvo', 'estaríamos', 'hemos', 'fuisteis', 'suyos', 'habéis', 'estar', 'haya', 'mía', 'habrían', 'estuviéramos', 'es', 'hayas', 'estaría', 'nuestras', 'muy', 'tendrán', 'serías', 'desde', 'quien', 'tenidos', 'tenías', 'tengo', 'fueras', 'seremos', 'serían', 'estuviésemos', 'estarás', 'hubieseis', 'estuve', 'o', 'nada', 'vuestros', 'algunos', 'seríais', 'otro', 'eso', 'tengáis', 'erais', 'estaban', 'tuve', 'tuvisteis', 'estos', 'tenía', 'sobre', 'este', 'fueseis', 'hubiste', 'ti', 'tuya', 'estaréis', 'hubisteis', 'estarían', 'esta', 'habidas', 'los', 'unos', 'estén', 'tuyos', 'sois', 'tenéis', 'seamos', 'tendría', 'habrás', 'que', 'estéis', 'todo', 'ya', 'habido', 'tuviéramos', 'habrá', 'fueran', 'entre', 'sus', 'será', 'mi', 'había', 'tengas', 'estaremos', 'antes', 'siente', 'fueses', 'estuviese', 'tuviésemos', 'cuando', 'estuviste', 'tendré', 'hasta', 'ni', 'habiendo', 'estés', 'tendremos', 'poco', 'estábamos', 'cual', 'sería', 'habíais', 'teniendo', 'seré', 'habrías', 'sin', 'estemos', 'un', 'como', 'vosotros', 'estuvierais', 'vuestro', 'fui', 'tendrían', 'lo', 'sean', 'mis', 'mío', 'tendrás', 'estuviera', 'tened', 'fuese', 'hubimos', 'fueron', 'tuvieron', 'se', 'estando', 'habré', 'hubieras', 'tuviera', 'ellas', 'le', 'esté', 'tuvieseis', 'hubieron', 'para', 'las', 'esto', 'estabais', 'fuesen', 'sintiendo', 'ellos', 'porque', 'sentida', 'de', 'del', 'fuerais', 'estáis', 'hayan', 'él', 'vosotras', 'está', 'están', 'hayáis', 'estaríais', 'éramos', 'uno', 'tuyas', 'fuésemos', 'suyas', 'habréis', 'hube', 'tenemos', 'habían', 'mí', 'tenidas'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWQ-UW-CC87r"
      },
      "source": [
        "# permite verificar en nltk si un token pertenece a diccionario de un idioma, en este caso a 'english'\n",
        "from nltk.corpus import words as voc_en\n",
        "x = len(voc_en.words())\n",
        "print('tamaño del diccionario en ingles del nltk: ',x)\n",
        "# verifica si una palabra pertenece al diccionario:\n",
        "w = \"house\"\n",
        "if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words):\n",
        "    print(w,\" true\")\n",
        "else:\n",
        "    print(w,\" false\")\n",
        "    \n",
        "w = \"pepito\"\n",
        "if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words):\n",
        "    print(w,\" true\")\n",
        "else:\n",
        "    print(w,\" false\")    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD2aCS5_C87r"
      },
      "source": [
        "# leer un archivo de ejemplo en .txt\n",
        "input_file = open(path_in+filenametxt, \"r\", encoding='iso-8859-1')\n",
        "filedata = input_file.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc2S9KMMC87s"
      },
      "source": [
        "# opción 1:\n",
        "# TOKENIZAR con .split(), \n",
        "# ELIMINAR tokens de long = 1\n",
        "# ELIMINAR caracteres que no sean alfanumericos y pasar todo a minuscula\n",
        "# REMOVER stop words con nltk\n",
        "# graficar los 20 términos más frecuentes:\n",
        "\n",
        "tokens = filedata.split()\n",
        "tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
        "# tokens=[word for word in tokens if word.isalpha()] si en vez de re.sub(r'[^A-Za-z0-9]+','',w) hace esto, que pasa?\n",
        "tokens = [w.lower() for w in tokens if len(w)>1]\n",
        "tokens = [w for w in tokens if w not in stop_words_nltk]\n",
        "\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "topwords = fdist.most_common(20)\n",
        "print (topwords)\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgxY2ADlC87s"
      },
      "source": [
        "# opción 2:\n",
        "# TOKENIZAR con nltk, \n",
        "# ELIMINAR tokens de long = 1\n",
        "# ELIMINAR caracteres que no sean alfanumericos\n",
        "# REMOVER stop words\n",
        "# graficar los 20 términos más frecuentes:\n",
        "\n",
        "tokens = nltk.word_tokenize(filedata)\n",
        "tokens = [w.lower() for w in tokens if len(w)>1]\n",
        "tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
        "tokens = [w for w in tokens if w not in stop_words_nltk]\n",
        "\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "topwords = fdist.most_common(20)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "print (topwords)\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbcOb-epC87s"
      },
      "source": [
        "# Stemming con NLTK\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "# probar cada una de las siguientes opciones: porter y lancaster.\n",
        "#tokens = [porter.stem(w) for w in tokens]\n",
        "tokens = [lancaster.stem(w) for w in tokens]\n",
        "\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "topwords = fdist.most_common(20)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwze-4h_C87s"
      },
      "source": [
        "# Lemmatization con NLTK\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# probar cada una de las siguientes opciones: \n",
        "#tokens = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in tokens ]\n",
        "tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens ]\n",
        "\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "topwords = fdist.most_common(20)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDlYGgXGC87t"
      },
      "source": [
        "# volver a leer el archivo ejemplo en .txt\n",
        "#input_file = open(path_in+filenametxt, \"r\",encoding='iso-8859-1')\n",
        "input_file = open(path_in+filenametxt, \"r\")\n",
        "output_file_clean = open(path_out+filenamecleantxt, \"w\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D30WuI_DC87t"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for line in input_file:\n",
        "    line_clean = \"\"\n",
        "    tokens = nltk.word_tokenize(line)\n",
        "    tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
        "    tokens = [w.lower() for w in tokens if len(w)>1]\n",
        "    tokens = [w for w in tokens if w.isalpha()]\n",
        "    tokens = [w for w in tokens if w not in stop_words_nltk]\n",
        "    #tokens = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in tokens]\n",
        "    tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens]\n",
        "\n",
        "    #tokens = [porter.stem(w) for w in tokens]\n",
        "    tokens = [lancaster.stem(w) for w in tokens]\n",
        "    \n",
        "    for w in tokens:\n",
        "        line_clean=line_clean+w+\" \"\n",
        "            \n",
        "    if (line_clean!=\"\"):\n",
        "        line_clean=line_clean+\"\\n\"\n",
        "        output_file_clean.write(line_clean)\n",
        "output_file_clean.close()        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JUtFmMbC87t"
      },
      "source": [
        "input_file_clean = open(path_out+filenamecleantxt, \"r\", encoding='iso-8859-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWSn8ETKC87t"
      },
      "source": [
        "filedata = input_file_clean.read()\n",
        "tokens = filedata.split()\n",
        "fdist = nltk.FreqDist(tokens)\n",
        "topwords = fdist.most_common(20)\n",
        "print('numero de palabras finales = ',len(fdist))\n",
        "x,y = zip(*topwords)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44vplUAqC87t"
      },
      "source": [
        "word_freq = fdist.most_common(len(fdist))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0Q0BNUFC87u"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open(path_out+filenamecsv, 'w') as csvFile:\n",
        "    writer = csv.writer(csvFile)\n",
        "    writer.writerow([\"word\", \"frecuency\"])\n",
        "    writer.writerows(word_freq)\n",
        "\n",
        "csvFile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFcJ_GCjC87u"
      },
      "source": [
        "# extract top 30 words\n",
        "top_words = word_freq[:20]\n",
        "print(top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxucCDg6C87u"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(top_words)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71bqjKCiC87v"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x,y = zip(*top_words)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BvEEisiC87v"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df = pd.DataFrame(top_words)\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.bar(df[0],df[1])\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"Word\")\n",
        "plt.ylabel(\"frecuency\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX8Ixpc6C87v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}